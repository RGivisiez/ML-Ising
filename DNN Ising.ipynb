{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys, os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HDF5 utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_name(name):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_paths(folder, data):\n",
    "    \n",
    "    sub_folders = []\n",
    "    data[folder].visit(sub_folders.append)\n",
    "    \n",
    "    path = []\n",
    "    \n",
    "    for sub_folder in sub_folders:\n",
    "        path.append(folder + '/' + sub_folder)\n",
    "    \n",
    "    return path[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_valid_index(max_size, train_size, valid_size):\n",
    "    \n",
    "    all_idx = np.arange(max_size)\n",
    "    \n",
    "    train_idx = np.random.choice(all_idx, size=train_size, replace=False)\n",
    "    \n",
    "    # Get the indexs that it ins't in train_idx\n",
    "    diff_idx = np.asarray(list(set(all_idx).difference(set(train_idx))))\n",
    "    \n",
    "    valid_idx = np.random.choice(diff_idx, size=valid_size, replace=False)\n",
    "    \n",
    "    # Index used so far\n",
    "    union_idx = set(valid_idx).union(set(train_idx))\n",
    "    \n",
    "    # Get the indexs that weren't used yet.\n",
    "    test_idx = np.asarray(list(set(all_idx).difference(union_idx)))\n",
    "    \n",
    "    size = train_idx.size + test_idx.size + valid_idx.size\n",
    "    \n",
    "    if size != max_size :\n",
    "        print('A soma dos tamanhos dos dataset é diferente do tamanho máximo')\n",
    "    \n",
    "    return train_idx, test_idx, valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only (r), read and write (r+)\n",
    "data = h5py.File('../Data/ising_conf-ener.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read attributes, thery are almost the same for all folders.\n",
    "data_attrs = [x for x in data['L=60'].attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lattice size\n",
    "L = data[main_folder].attrs['Lattice Size']\n",
    "\n",
    "# Main folder name (L=60,90,120)\n",
    "main_folder = 'L=' + str(L)\n",
    "\n",
    "# Critical temperature\n",
    "tc = data[main_folder].attrs['Critical Temperature']\n",
    "\n",
    "# Critical temperature conf-ener folder\n",
    "tc_path = '{0}/T={1:6.4f}/conf-ener'.format(main_folder, tc) \n",
    "\n",
    "# All lattice sizes\n",
    "lattice_size = data[main_folder].attrs['Lattice Size']\n",
    "\n",
    "# All temperatures in the main folder\n",
    "temperatures = data[main_folder].attrs['Temperatures']\n",
    "temperatures = np.delete(temperatures, np.where(temperatures==tc))\n",
    "\n",
    "# Paths to the datasets in main folder\n",
    "dataset_paths = get_dataset_paths(main_folder, data)\n",
    "dataset_paths.remove(tc_path)\n",
    "\n",
    "# Monte Carlo steps\n",
    "mc_steps = np.int32(data[main_folder].attrs['Monte Carlo Steps'])\n",
    "\n",
    "# Dataset total size\n",
    "dataset_size = temperatures.size * mc_steps\n",
    "\n",
    "# Train percet\n",
    "train_perc = 0.95\n",
    "\n",
    "# Validation percet\n",
    "valid_perc = 0.02\n",
    "\n",
    "# Test percet\n",
    "test_perc = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = np.ceil(mc_steps * train_perc).astype(np.int32)\n",
    "valid_size = np.ceil(mc_steps * valid_perc).astype(np.int32)\n",
    "test_size = mc_steps - (valid_size+train_size)\n",
    "\n",
    "train_idx = np.zeros((train_size, len(dataset_paths)), dtype=np.int32)\n",
    "valid_idx = np.zeros((valid_size, len(dataset_paths)), dtype=np.int32)\n",
    "test_idx = np.zeros((test_size, len(dataset_paths)), dtype=np.int32)\n",
    "\n",
    "for idx, _ in enumerate(dataset_paths):\n",
    "    train_idx[:, idx], test_idx[:, idx], valid_idx[:, idx] = get_train_test_valid_index(mc_steps, train_size,\n",
    "                                                                                        valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040000000000000036 13\n",
      "0.14000000000000012 14\n",
      "0.2400000000000002 15\n",
      "0.3400000000000003 16\n",
      "0.4400000000000004 17\n",
      "0.5400000000000005 18\n",
      "0.6400000000000006 19\n",
      "0.7400000000000007 20\n",
      "0.8400000000000007 21\n",
      "0.9400000000000008 22\n",
      "1.040000000000001 23\n",
      "1.140000000000001 24\n",
      "1.240000000000001 25\n",
      "1.3400000000000012 26\n",
      "1.4400000000000013 27\n",
      "1.5400000000000014 28\n",
      "1.6400000000000015 29\n"
     ]
    }
   ],
   "source": [
    "data_slice = np.floor(train_size * 0.1).astype(np.int32)\n",
    "\n",
    "train_batch = np.zeros((data_slice * temperatures.size, L * L), dtype=np.int16)\n",
    "\n",
    "label_batch = np.zeros(data_slice * temperatures.size, dtype=np.int16)\n",
    "\n",
    "for idx, dataset_path in enumerate(dataset_paths):\n",
    "    \n",
    "    train_set = data[dataset_path]['Configuration']\n",
    "    \n",
    "    train_batch[idx * data_slice: (idx + 1) * data_slice, :] = train_set[0, train_idx[:data_slice, idx]]\n",
    "    \n",
    "    if temperatures[idx] - 2.26 > 0.0:\n",
    "        label_batch[idx * data_slice: (idx + 1) * data_slice] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_batch(data, train_idx):\n",
    "    train = np.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=( L * L,), activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100)               360100    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 360,201\n",
      "Trainable params: 360,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=,\n",
    "                   steps_per_epoch = int(3800 // batch_size),\n",
    "                   epochs = 10,\n",
    "                   verbose = 1,\n",
    "                   validation_data = ,\n",
    "                   validation_steps = int(950 // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
